{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "import os\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import traci\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether SUMO was properly installed\n",
    "if 'SUMO_HOME' not in os.environ:\n",
    "    print(\"Entered\")\n",
    "    os.environ['SUMO_HOME'] = \"/path/to/sumo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dictionary maps each traffic light intersection to the id's (From Netedit in SUMO) of the incoming lanes\n",
    "lanes = {\n",
    "    \"261199942\" : [\"435326187#0\", \"-377882482#1\", \"-435677953#2\", \"-435326192#1\"], \n",
    "    \"261174045\" : [\"435677953#0\", \"1303785156#0\", \"-435677955#1\", \"-57782117#3\"]  \n",
    "}\n",
    "\n",
    "#This dictionary is used to compute the differences in metric values between the current and previous observations by storing the previous observation's values\n",
    "previousMetrics = {\"435326187#0\": [], \"-377882482#1\": [], \"-435677953#2\": [], \"-435326192#1\": [], \"435677953#0\": [], \"1303785156#0\": [], \"-435677955#1\": [], \"-57782117#3\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, the custom SUMORL environment used to interact with SUMO simulations, extract observations, and compute metrics and rewards is defined. This environment was used to both train and test the single agent PPO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import traci\n",
    "import xml.etree.ElementTree as ET\n",
    "from stable_baselines3 import PPO, A2C\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#SumoRL class\n",
    "class SumoRL(gym.Env):\n",
    "    def __init__(self, net_file, route_file_list): #constructor of SumoRL\n",
    "        super(SumoRL, self).__init__()\n",
    "\n",
    "        # Initialize SUMO with net and route files\n",
    "        self.net_file = net_file\n",
    "        self.route_file_list = route_file_list\n",
    "        self.route_file_index = -1\n",
    "        self.route_file = \"\"\n",
    "        self.isTest = not bool(self.route_file_list)\n",
    "        self.reward_values = []\n",
    "        self.queue_lengths = []\n",
    "        self.traffic_densities = []\n",
    "        self.average_speeds = []\n",
    "        self.simulation_started = False\n",
    "\n",
    "        \n",
    "        # e.g., [green, yellow, red duration for each signal]\n",
    "        self.action_space = spaces.Box(low=2, high=45, shape=(4,), dtype=np.float64)  # Max duration per signal phase (red and yellow)\n",
    "        self.metrics = spaces.Box(low=0, high=100, shape=(3,), dtype=int)\n",
    "        self.lane_index = spaces.Box(low=0, high=3, shape=(),dtype=int)\n",
    "        self.observation_space = spaces.Dict({ #The observation space for all incoming lanes, and includes beginning and ending timestamps as well\n",
    "         \"261199942_0\": self.metrics,\n",
    "         \"261199942_1\": self.metrics,\n",
    "         \"261199942_2\": self.metrics,\n",
    "         \"261199942_3\": self.metrics,\n",
    "         \"261174045_0\": self.metrics,\n",
    "         \"261174045_1\": self.metrics,\n",
    "         \"261174045_2\": self.metrics,\n",
    "         \"261174045_3\": self.metrics,\n",
    "         \"begin_time\": spaces.Box(low=0, high=86400, shape=(1,), dtype=int),\n",
    "         \"end_time\": spaces.Box(low=600, high=86400, shape=(1,), dtype=int)\n",
    "        })\n",
    "         \n",
    "        self.begin_time = -600\n",
    "        self.end_time = 0\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.compute_metrics()\n",
    "        \n",
    "    def step(self, actions):\n",
    "        \n",
    "        timing = {\"261199942\": [actions[0], actions[1]], \"261174045\": [actions[2], actions[3]]}\n",
    "\n",
    "        for intersection in lanes:\n",
    "            # Apply action\n",
    "            curr_logic = traci.trafficlight.getAllProgramLogics(intersection)[0]\n",
    "            curr_logic.phases[1].duration = timing[intersection][0]\n",
    "            curr_logic.phases[5].duration = timing[intersection][1]\n",
    "            traci.trafficlight.setProgramLogic(intersection, curr_logic)\n",
    "            \n",
    "        \n",
    "        if self.isTest:\n",
    "            count = 0\n",
    "            while count < 300:\n",
    "                traci.simulationStep()\n",
    "                count+=1\n",
    "        else:\n",
    "            traci.simulationStep() # Perform one simulation step in SUMO (prior to extracing the next observation from the simulation)\n",
    "\n",
    "        result_obs = self.get_observation()\n",
    "\n",
    "        info = {} #Used to log additional info\n",
    "\n",
    "        rewards = self.compute_reward(result_obs)\n",
    "        print(f'Reward: {rewards}')\n",
    "\n",
    "        done = traci.simulation.getMinExpectedNumber() == 0 #check termination of simulation\n",
    "\n",
    "        return result_obs, rewards, done, info\n",
    "  \n",
    "    def reset(self):  #invoked every time the previous SUMO simulation episode terminates (or at the very beginning of the simulation to get the first observation) \n",
    "       \n",
    "        if self.isTest: #reset handling for testing phase\n",
    "            if self.simulation_started:\n",
    "              traci.close()  # Close the current session\n",
    "            traci.start(['sumo', '-c', 'sumo_config_testing.sumocfg'], label=\"13221112112242433143322511234456754342545744365\")  # Restart simulation (run headless to improve efficiency)\n",
    "            self.simulation_started = True\n",
    "            return self.get_observation()\n",
    "\n",
    "        #reset handling for training phase begins here\n",
    "     \n",
    "        if self.simulation_started:\n",
    "            traci.close()  \n",
    "        \n",
    "        #Update the begin and end times for the next SUMO simulation episode to include in each observation\n",
    "        self.begin_time = self.end_time\n",
    "        self.end_time += 600\n",
    "        if self.begin_time == 86400:\n",
    "           self.begin_time = 0\n",
    "           self.end_time = 600\n",
    "        tree = ET.parse(\"sumo_config.sumocfg\")\n",
    "        root = tree.getroot()\n",
    "        self.route_file_index+=1 #update route file index\n",
    "        if self.route_file_index >= len(self.route_file_list):\n",
    "            self.route_file_index = 0 #restart from the beginning\n",
    "        self.route_file = self.route_file_list[self.route_file_index] #update the route file being used\n",
    "        \n",
    "        input_element = root.find(\"input\") #load new route file for next SUMO simulation episdoe\n",
    "        input_element.find(\"route-files\").set('value', self.route_file)\n",
    "        \n",
    "        \n",
    "        tree.write(\"sumo_config.sumocfg\") #save changes\n",
    "        traci.start(['sumo', '-c', 'sumo_config.sumocfg'], label=\"3232211121112223412232222551145246276225\")  # Restart simulation (run headless to improve efficiency)\n",
    "        self.simulation_started = True\n",
    "        return self.get_observation()\n",
    "     \n",
    "     #For each incoming lane, the observation metrics are extracted using traci API \n",
    "    def compute_metrics(self):\n",
    "        \n",
    "        observation = {\"261199942_0\": [], \"261199942_1\": [], \"261199942_2\": [], \"261199942_3\": [],\"261174045_0\": [], \"261174045_1\": [], \"261174045_2\": [], \"261174045_3\": []}\n",
    "        for intersection in lanes:\n",
    "         for edge in lanes[intersection]:\n",
    "           # print(\"Entered lane loop\")\n",
    "            i = str(lanes[intersection].index(edge))\n",
    "           \n",
    "            queue_length=traci.edge.getLastStepVehicleNumber(edge)\n",
    "            vehicle_ids = traci.edge.getLastStepVehicleIDs(edge)\n",
    "            traffic_density=(len(vehicle_ids) / traci.lane.getLength(list(filter(lambda lane: edge in lane, traci.lane.getIDList()))[0]))\n",
    "                  \n",
    "            \n",
    "           \n",
    "            observation[intersection + \"_\" + i].append(queue_length)\n",
    "            observation[intersection + \"_\" + i].append(traffic_density)\n",
    "            if vehicle_ids:\n",
    "              total_speed = sum([traci.vehicle.getSpeed(vehicle_id) for vehicle_id in vehicle_ids])\n",
    "              observation[intersection + \"_\" + i].append((total_speed / len(vehicle_ids)))\n",
    "\n",
    "            else:\n",
    "                observation[intersection + \"_\" + i].append(0)\n",
    "\n",
    "       \n",
    "        observation['begin_time'] = np.array([self.begin_time])\n",
    "        observation['end_time'] = np.array([self.end_time])\n",
    "    \n",
    "        return observation\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass  # SUMO's own GUI can be used if required\n",
    "\n",
    "    def close(self):\n",
    "        # Close the simulation (if needed)\n",
    "        traci.close()\n",
    "\n",
    "#Rewards are computed for each observation, which involves iterating through each incoming lanes of all traffic light intersections\n",
    "    def compute_reward(self, observation):\n",
    "        rewardValue = 0\n",
    "        total_queue_length = 0\n",
    "        total_traffic_density = 0\n",
    "        total_average_speed = 0\n",
    "        isInitial = False\n",
    "\n",
    "        for intersection in lanes:\n",
    "            for lane in lanes[intersection]:\n",
    "                i = str(lanes[intersection].index(lane))\n",
    "                queue_length = observation[intersection + \"_\" + i][0]\n",
    "                traffic_density = observation[intersection + \"_\" + i][1]\n",
    "                avg_speed = observation[intersection + \"_\" + i][2]\n",
    "                \n",
    "                \n",
    "                if(len(previousMetrics[lane]) == 0): #Observation of first iteration in simulation is initialized for future comparison to compute the reward\n",
    "                    isInitial = True\n",
    "                    previousMetrics[lane] = [queue_length, traffic_density, avg_speed]\n",
    "                    break\n",
    "                #Take the summation of the reward in each incoming lane\n",
    "                rewardValue+=1.5*(previousMetrics[lane][0] - queue_length) + 1.5*(previousMetrics[lane][1] - traffic_density) + 3*(avg_speed - previousMetrics[lane][2])\n",
    "                previousMetrics[lane] = [queue_length, traffic_density, avg_speed] #store the metric values of the previous observation for future reward computation\n",
    "\n",
    "                \n",
    "                total_queue_length+=queue_length\n",
    "                total_traffic_density+=traffic_density\n",
    "                total_average_speed+=avg_speed\n",
    "            \n",
    "            if isInitial:\n",
    "                break\n",
    "        \n",
    "        #Metric values are accumulated in separate lists to later plot the data\n",
    "        self.reward_values.append(rewardValue)\n",
    "        self.queue_lengths.append(total_queue_length/8)\n",
    "        self.traffic_densities.append(total_traffic_density/8)\n",
    "        self.average_speeds.append(total_average_speed/8)\n",
    "\n",
    "        return rewardValue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below is the training method, where the training route rou.xml files are loaded, the SUMORL environment is initialized, and the single agent PPO model is instantiated with optimally chosen hyperparamter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(): #method used for training phase\n",
    "  \n",
    "    traci.start(['sumo', '-c', 'sumo_config.sumocfg'], label=\"5721222111423332123221323344242665425544543171\")\n",
    "\n",
    "    print(\"Init env...\")\n",
    "    route_file_list = [] \n",
    "    \n",
    "    #All training files are appended to account for 7 days worth of data extracted from the Chicago traffic database.\n",
    "    for day in range(0, 7):\n",
    "        for time_index in range(1, 145):\n",
    "            route_file_list.append(f'new_sumo_route-file_at_time_index_{time_index}_for_day_{day}.rou.xml')\n",
    "    \n",
    "    env = SumoRL('sumo_network_file_12_33.net.xml', route_file_list)\n",
    "\n",
    "    model = PPO( # Initialize PPO agent from Stable-Baselines3\n",
    "        policy=\"MultiInputPolicy\",\n",
    "        env=env,\n",
    "        vf_coef=0.75,\n",
    "        verbose=1,\n",
    "        learning_rate=0.0003,\n",
    "        n_steps=2048,\n",
    "        batch_size=256,\n",
    "        n_epochs=7,\n",
    "        max_grad_norm=0.3,\n",
    "        gamma=0.99,\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(\"Beginning the training of the PPO agent...\")\n",
    "    total_timesteps = 201600\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    metrics = {\"Reward\": env.reward_values, \"Queue length\": env.queue_lengths, \"Traffic density\": env.traffic_densities, \"Average speed\": env.average_speeds}\n",
    "    \n",
    "    #Plot graphs \n",
    "    \n",
    "    for metric_key in metrics:\n",
    "      print(len(metrics[metric_key]))\n",
    "      print(f'Average  {metric_key}: {sum([value for value in metrics[metric_key]])/len(metrics[metric_key])}')\n",
    "      plt.plot(list(range(1, len(metrics[metric_key]) + 1)), metrics[metric_key])\n",
    "      plt.xlabel(\"Timestep\")\n",
    "      plt.ylabel(metric_key)\n",
    "      plt.title(f'{metric_key} at each Timestep')\n",
    "      plt.show()\n",
    "\n",
    "    #Save model results\n",
    "    model_path = \"ppo_model_sumo_single_agent_trained\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    " \n",
    "    \n",
    "    traci.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method below is used to test the trained model using a 24-hour SUMO simulation that leverages a different route file, and uses a separate\n",
    "sumocfg file. The metrics of the results are also plotted. For testing the baseline, just the action logic was commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_main(): #method used for testing phase\n",
    "    traci.start(['sumo', '-c', 'sumo_config_testing.sumocfg'], label=\"5422142332122124132212443513453443223524235224563\")\n",
    "\n",
    "    print(\"Init env...\")\n",
    "    \n",
    "    env = SumoRL('sumo_network_file_12_33.net.xml', [])\n",
    "\n",
    "    print(\"Testing the PPO agent...\")\n",
    "    model = PPO.load(\"ppo_model_sumo_single_agent_trained\") #load the trained PPO model\n",
    "    observation = env.reset()\n",
    "    for timestep in range(1, 289):\n",
    "           actions,_ = model.predict(observation , deterministic = True) #the deterministic flag was set to true instead of false to signify the testing stage\n",
    "           observation, rewards, done, info = env.step(actions)\n",
    "           if done:\n",
    "               observation = env.reset()\n",
    "    \n",
    "    metrics = {\"Reward\": env.reward_values, \"Queue length\": env.queue_lengths, \"Traffic density\": env.traffic_densities, \"Average speed\": env.average_speeds}\n",
    "   \n",
    "    #Plot graphs for metrics\n",
    "    for metric_key in metrics:\n",
    "      print(len(metrics[metric_key]))\n",
    "      print(f'Average  {metric_key}: {sum([value for value in metrics[metric_key]])/len(metrics[metric_key])}')\n",
    "      plt.plot(list(range(0, len(metrics[metric_key]))), metrics[metric_key])\n",
    "      plt.xlabel(\"Timestep\")\n",
    "      plt.ylabel(metric_key)\n",
    "      plt.title(f'{metric_key} at each Timestep')\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   train_main()\n",
    "   test_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
